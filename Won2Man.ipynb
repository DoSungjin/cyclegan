{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anwiu3MzD3U-"
      },
      "source": [
        "# Woman to Man Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE6gnRCvcYAP",
        "outputId": "7f74dbc5-15c0-4346-fb37-a65c11a77c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUvSSLIuD3VB"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBTEQsGiD3VC"
      },
      "source": [
        "**Code 1. Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEhEDKkmvn-M"
      },
      "source": [
        "Image to Image Translation (style transfer 일종)\n",
        " \n",
        " -cycle GAN 구조 -> 진짜 이미지를 통해 가짜 이미지를 생성한 후 계속해서 비교하면서 진짜 이미지와 가짜 이미지를 판별하면서 반복한다.\n",
        "\n",
        "가짜 이미지 B^과 A^그리고 판별할 Da, Db가 필요하다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLPrsJETny3S",
        "outputId": "b41c7854-65f2-47ce-fb63-dc7f1f7312c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/python/data/Dataset\n"
          ]
        }
      ],
      "source": [
        "cd gdrive/MyDrive/python/data/'Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZvu_E8JqIXu",
        "outputId": "e71c01a0-a179-4185-d6ac-a52c62890bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /dlib-19.18.0-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/dlib-19.18.0-cp37-cp37m-linux_x86_64.whl'\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V3Jc_iFD3VC"
      },
      "outputs": [],
      "source": [
        "def to_rgb(image):\n",
        "    rgb_image = Image.new(\"RGB\", image.size)\n",
        "    rgb_image.paste(image)\n",
        "    return rgb_image\n",
        "    #PIL의 Image모듈을 통해 흑백이미지를 RGB로 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iODjyYXxjF_"
      },
      "outputs": [],
      "source": [
        "# # Python Image Library (pillow모듈)\n",
        "#  주요기능에는 픽셀단위 조작/ 흐림, 윤곽보정 등 이미지 필터/ 선명하게, 밝기보정, 명암보정, 색보정 등 회상조정/ 이미지에 텍스트 추가 등\n",
        "#  설치는 pip install pillow\n",
        "#  from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjBBLhG8D3VC"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_= None, unaligned=False, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.unaligned = unaligned\n",
        "        # train 모드일 때는 trainA, trainB에 있는 디렉토리에서 이미지를 불러옵니다. \n",
        "        # unaligned = False는 학습할쌍을 고정시키지 않겠다는 뜻입니다.(무작위)\n",
        "        # transform은 PIL이미지를 토치의 tensor자료형으로 바꿉니다.\n",
        "    \n",
        "            # glob 함수로 trainA 디렉토리의 이미지의 목록을 불러옵니다. \n",
        "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*')) #'/*,*'은 모든 파일을 뜻합니다. *.jpg로 파일 명을 설정할 수도 있다.\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
        "        # if mode =\"train\": 으로 트레인 폴더의 A,B와 테스트 폴더의 A,B를 나누어도 되지만 같은 폴더에 넣는 것으로 '%sA'로 코드 간결화\n",
        "    def __getitem__(self, index):\n",
        "        # index값으로 이미지의목록 중 이미지 하나를 불러옵니다. \n",
        "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
        "        # unaligned 변수로 학습할 Pair를 랜덤으로 고릅니다.  \n",
        "        if self.unaligned:\n",
        "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
        "        else:\n",
        "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
        "       \n",
        "        # Convert grayscale images to rgb\n",
        "        if image_A.mode != \"RGB\":\n",
        "            image_A = to_rgb(image_A)\n",
        "        if image_B.mode != \"RGB\":\n",
        "            image_B = to_rgb(image_B)\n",
        "        item_A = self. transform(image_A)\n",
        "        item_B = self. transform(image_B)\n",
        "        # transform을 sorted앞에 붙여서 간결화 가능? 복잡해서 이게 더 깔끔?\n",
        "\n",
        "        return {'A': item_A, 'B':item_B}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))\n",
        "\n",
        "        # 앞으로 loader을 이용하여 배치사이즈 만큼 이미지를 불러올 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ8adjM9D3VD"
      },
      "source": [
        "**Code 2. Generator & Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjgaYezED3VE"
      },
      "outputs": [],
      "source": [
        "# [가중치 초기화 함수]\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if hasattr(m, \"bias\") and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "        # 여기 함수는 layer의 종류에 따라 가중치를 초기화 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPgCU8CiD3VE"
      },
      "outputs": [],
      "source": [
        "# Gnerator 구현을 위해 내부에 들어갈 la yor인 Residual Block을 구현합니다.\n",
        "# Residual Block은 이전 layer와 현재 layer의 출력값을 더해서 foward 시키는 것을 기울기 소실 문제를 해결합니다.(더하기 연산으로는 작아지지 않으니)\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1), #점대칭방식, 가장 가까운 픽셀로 값복사(자연스런 이미지 생성) vs zero padding은 값을 지정합니다.\n",
        "            nn.Conv2d(in_features, in_features, 3), \n",
        "            nn.InstanceNorm2d(in_features), #데이터 개별로 정규화를 진행합니다.\n",
        "            # 정규화는 데이터 값 범위 조정을 하는 것으로 배치정규화는 배치단위로 진행합니다. instance 정규화는 이미지에 특화된 정규화 과정으로 이미지 개별에 정규화를 진행합니다.\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x) #입력 feature개수와 출력 feature개수를 일치시켜야 합니다.\n",
        "        # Residual Block이 늘어날수록 더 많은 계층적인 정보가 담겨 더욱 그럴듯한 이미지가 생성됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUU_6ZSRD3VF"
      },
      "outputs": [],
      "source": [
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, input_shape, num_residual_blocks):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        channels = input_shape[0]\n",
        "\n",
        "        # Initial convolution block(초기의 convolution block 설정)\n",
        "        out_features = 64\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(channels, out_features, 7),\n",
        "            nn.InstanceNorm2d(out_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        in_features = out_features\n",
        "\n",
        "        # Downsampling을 2번하는 경우로 stride가 2로 이미지 크기를 반씩 줄입니다.\n",
        "        for _ in range(2):\n",
        "            out_features *= 2\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(num_residual_blocks):\n",
        "            model += [ResidualBlock(out_features)]\n",
        "            # num_risidual_blocks 만큼 residual block을 만듭니다. (입력 출력 일치)\n",
        "\n",
        "        # Upsampling을 2번하면서 2배씩 크기를 늘립니다.\n",
        "        for _ in range(2):\n",
        "            out_features //= 2\n",
        "            model += [\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_features = out_features\n",
        "\n",
        "        # Output layer을 선언합니다. /입력 이미지 크기 = 출력 이미지 크기 / 마지막 층 활성화 함수는 nn.Tanh()입니다.\n",
        "        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q867yZJcD3VF"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        channels, height, width = input_shape\n",
        "\n",
        "        # Calculate output shape of image discriminator (PatchGAN) \n",
        "        # discriminator의 출력 크기를 정의 합니다.\n",
        "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            # block은 stride 2로 점점 downsampling하며 출력 이미지 크기를 줄입니다.\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "            \n",
        "        # block 4번을 통과합니다. 그래서 16x16이 됩니다. (한번 통과시 반이 줄어듦) \n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, 64, normalize=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "#  patch GAN의 Discriminator는 출력이 0이나 1의 값이 아닌 입력 이미지의 1/16인 이진화된 feature map이 나옵니다. \n",
        "  # if 이미지 크기가 256x256이면 patch GAN의 Discriminator의 출력 16x16으로 나옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3bcwjWAE5Uu"
      },
      "source": [
        "What is a feature map in CNN?\n",
        "\n",
        "\n",
        "The feature maps of a CNN capture the result of applying the filters to an input image. I.e at each layer, the feature map is the output of that layer. The reason for visualising a feature map for a specific input image is to try to gain some understanding of what features our CNN detects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTg5cZT7D3VG"
      },
      "source": [
        "**Code 3. Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoYmnt82IycN"
      },
      "outputs": [],
      "source": [
        "class opt :\n",
        " pretrained_model_path = \"/content/gdrive/MyDrive/python/data/Dataset/saved_models/wom2man\"\n",
        " # epoch to start training from \n",
        " epoch_start = 130\n",
        " dataset_name=\"wom2man\" #여기서 폴더가 지정되어 있어서 dataset안에 oldtonew폴더를 생성해야한다.\n",
        " channels = 3  #흑백의 경우 1 /RGB이미지는 3\n",
        " img_height = 256 \n",
        " img_width = 256 #가로세로크기 / 크기를 키울경우 cuda 메모리 오류가 생긴다.\n",
        " n_residual_blocks=9 #Generator에서의 Residual Block의 개수\n",
        " lr=0.0002 #learning rate\n",
        " b1=0.5 \n",
        " b2=0.999  #b1과 b2는 Adam opimizer에 대한 Hyper Parameter\n",
        " n_epochs=400 #목표200/ 했다가 비슷한 이미지가 생성되는 것을 확인하여 다른 방법이 필요하다고 느낌 or 에폭을 더 늘려보기\n",
        " init_epoch=0\n",
        " decay_epoch=120 #목표 100 /학습속도를 조절 처음에는 크게 그담에는 작은 단위로\n",
        " lambda_cyc=10.0\n",
        " lambda_id=5.0 #cycle-consistency loss 와 identity loss에 대한 람다값(id가 클수록 본래 색감 유지)\n",
        " n_cpu=8\n",
        " batch_size=1\n",
        " sample_interval=300\n",
        " checkpoint_interval= 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nigN-cc1D3VG"
      },
      "outputs": [],
      "source": [
        "\n",
        "pretrained_model_path = \"/content/gdrive/MyDrive/python/data/Dataset/saved_models/wom2man\"\n",
        "# epoch to start training from \n",
        "epoch_start = 130\n",
        "dataset_name=\"wom2man\" #여기서 폴더가 지정되어 있어서 dataset안에 oldtonew폴더를 생성해야한다.\n",
        "channels = 3  #흑백의 경우 1 /RGB이미지는 3\n",
        "img_height = 256 \n",
        "img_width = 256 #가로세로크기 / 크기를 키울경우 cuda 메모리 오류가 생긴다.\n",
        "n_residual_blocks=9 #Generator에서의 Residual Block의 개수\n",
        "lr=0.0002 #learning rate\n",
        "b1=0.5 \n",
        "b2=0.999  #b1과 b2는 Adam opimizer에 대한 Hyper Parameter\n",
        "n_epochs=400 #목표200/ 했다가 비슷한 이미지가 생성되는 것을 확인하여 다른 방법이 필요하다고 느낌 or 에폭을 더 늘려보기\n",
        "init_epoch=0\n",
        "decay_epoch=120 #목표 100 /학습속도를 조절 처음에는 크게 그담에는 작은 단위로\n",
        "lambda_cyc=10.0\n",
        "lambda_id=5.0 #cycle-consistency loss 와 identity loss에 대한 람다값(id가 클수록 본래 색감 유지)\n",
        "n_cpu=8\n",
        "batch_size=1\n",
        "sample_interval=300\n",
        "checkpoint_interval= 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPMhxfYID3VG"
      },
      "outputs": [],
      "source": [
        "# Create sample and checkpoint directories \n",
        "#샘플 이미지와 모델 가중치를 저장할 폴더를 생성.\n",
        "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
        "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
        "#exist_ok=True면 같은 폴더가 있어도 오류가 나지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZJYfgTfD3VG"
      },
      "outputs": [],
      "source": [
        "# Losses(손실함수 정의)\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61O_n58oD3VH"
      },
      "outputs": [],
      "source": [
        "#모델 객체 선언하기\n",
        "input_shape = (channels, img_height, img_width)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n",
        "G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n",
        "D_A = Discriminator(input_shape)\n",
        "D_B = Discriminator(input_shape)\n",
        "# 스타일 A ->B :Generator G.AB\n",
        "# 스타일 B ->A :Generator G.BA\n",
        "# 스타일 A,B가 진짜인지 가짜인지 판별할 네트워크 D.a, D.b선언"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmjEVA21OuFm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTZ0SzttD3VH"
      },
      "outputs": [],
      "source": [
        "# GPU에 로드하기\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "if cuda:\n",
        "    G_AB = G_AB.cuda()\n",
        "    G_BA = G_BA.cuda()\n",
        "    D_A = D_A.cuda()\n",
        "    D_B = D_B.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_cycle.cuda()\n",
        "    criterion_identity.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA3RQHgID3VH"
      },
      "outputs": [],
      "source": [
        "if epoch_start != 0:\n",
        "    # Load pretrained models \n",
        "    G_AB.load_state_dict(torch.load(f\"{pretrained_model_path}/G_AB_130.pth\")) #런타임 오류로 끊길 우려가 있어서 중간에 가중치를 로드받아서 진행하는 모델 생성\n",
        "    G_BA.load_state_dict(torch.load(f\"{pretrained_model_path}/G_BA_130.pth\"))\n",
        "    D_A.load_state_dict(torch.load(f\"{pretrained_model_path}/D_A_130.pth\"))\n",
        "    D_B.load_state_dict(torch.load(f\"{pretrained_model_path}/D_B_130.pth\"))\n",
        "  \n",
        "\n",
        "\n",
        "# Initialize weights\n",
        "else : \n",
        "    G_AB.apply(weights_init_normal)    # old=>new generator 가중치 초기화\n",
        "    G_BA.apply(weights_init_normal)    # new=>old generator 가중치 초기화\n",
        "    D_A.apply(weights_init_normal)    # old discriminator 가중치 초기화\n",
        "    D_B.apply(weights_init_normal)    # new discriminator 가중치 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0zvlioCD3VI"
      },
      "outputs": [],
      "source": [
        "# Optimizers 정의\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n",
        ")\n",
        "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n",
        "# 앞서 정의한 hyper parameter lr,b1,b2로 optimizer를 정의합니다. \n",
        "# intertools,chain을 이용하면 optimizer가 여러 모델의 parameter를 하나의 모델을 다루는 것처럼 동작합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U9L9uZ1D3VI"
      },
      "outputs": [],
      "source": [
        "# 학습스케줄러\n",
        "class LambdaLR:\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
        "        # 람다LR클래스를 정의합니다. lr를 decay할 epoch을 정할 수 있습니다.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsrlaUo6D3VI"
      },
      "outputs": [],
      "source": [
        "# Learning rate update schedulers\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_G, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",
        ")\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_A, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",
        ")\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_B, lr_lambda=LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGYIziw3D3VI"
      },
      "outputs": [],
      "source": [
        "# 텐서 연산에 사용할 Tensor 자료형을 정의합니다.\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4I11N0rD3VI"
      },
      "outputs": [],
      "source": [
        "# Torch변수가 requires_grad=True로 지정되어 있다면 매 연산마다 Gradient를 저장하므로 \n",
        "# cycle GAN학습을 위해서는 ReplayBuffer클래스를 통해 이미지를 따로 저장해야 합니다.\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=50):\n",
        "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[i] = element\n",
        "                else:\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RyeqcUnD3VJ"
      },
      "outputs": [],
      "source": [
        "# Buffers of previously generated samples\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v-VyMpUD3VJ",
        "outputId": "c8897eb5-f64a-4707-d3bd-94a4928efe00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        }
      ],
      "source": [
        "# Dataset클래스로 이미지를 불러오는 방식에 대하여 정의합니다.\n",
        "# Image transformations\n",
        "transforms_ = [\n",
        "    transforms.Resize(int(img_height * 1.12), Image.BICUBIC), #PIL이미지를 가로, 세로로 1.12배 확대합니다. 이때 보간 방식은 BICUBIC을 사용합니다.\n",
        "    transforms.RandomCrop((img_height, img_width)), #PIL이미지를 가로, 세로 길이만큼 무작위로 잘라냅니다.\n",
        "    transforms.RandomHorizontalFlip(),#PIL이미지를 무작위로 좌우로 뒤집습니다,\n",
        "    transforms.ToTensor(), #PIL이미지를 0~1사이의 Tensor자료형으로 변환합니다.\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #RGB 채널별로 픽셀값이 평균 0.5, 표준편차가 0.5가 되도록 정규화합니다.\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t71s5fTHMbLK"
      },
      "source": [
        "학습 혹은 평가 중 이미지 데이터를 불러올 Dataloader를 정의합니다. 앞서 정의한 image dataset클래스로 selfie2anime데이터 폴더로부터 transform_를 적용한 이미지를 배치 사이즈만큼 불러옵니다. 또한 num_workers로 cpu유틸리티를 설정할 수 있습니다. shuffle변수를 True 혹은 False로 설정해 이미지를 무작위로 혹은 순차적으로 불러올 수 있습니다. colab에서 실습할 경우 GPU 메모리 부족으로 OOM이슈가 나올 수 있으므로 배치사이즈를 줄여 실행 하면 됩니다.\n",
        "\n",
        "OutOfMemory Error\n",
        "OOM의 원인은 다양합니다. 반드시 Memory Leak이 아닐 수도 있습니다. 처리 메모리 부족?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJV7MhIkD3VJ",
        "outputId": "c30eae1c-6d7b-4689-d58f-4703ecf75c91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# Training data loader\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset(\"/content/gdrive/MyDrive/python/data/Dataset/images/%s\" % dataset_name, transforms_=transforms_, unaligned=True),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu,\n",
        ")\n",
        "\n",
        "# Test data loader\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(\"/content/gdrive/MyDrive/python/data/Dataset/images/%s\" % dataset_name, transforms_=transforms_, unaligned=True, mode=\"test\"),\n",
        "    batch_size=5,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex8BGZbeNumS"
      },
      "source": [
        "**생성한 샘플이미지를 시각화하여 저장하는 함수를 지정**\n",
        "합슥하는 동안 생성한 이미지를 확인 및 저장하기 위해 sample_images함수를 정의합니다. val_dataloader의 배치 사이즈가 5이므로 sample_images는 make_grid함수를 통해 5개씩 샘플을 생성하여 images폴더에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9OxVOLUD3VJ"
      },
      "outputs": [],
      "source": [
        "def sample_images(batches_done):\n",
        "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    G_AB.eval()\n",
        "    G_BA.eval()\n",
        "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
        "    fake_B = G_AB(real_A)\n",
        "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
        "    fake_A = G_BA(real_B)\n",
        "    # Arange images along x-axis\n",
        "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
        "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
        "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
        "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
        "    # Arange images along y-axis\n",
        "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
        "    save_image(image_grid, \"images/%s/%s.png\" % (dataset_name, batches_done), normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0_NWkWID3VJ",
        "outputId": "f81bbf25-bf89-4ba8-bfdf-880c7d7468d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 399/400] [Batch 72/73] [D loss: 0.046236] [G loss: 1.743282, adv: 0.732759, cycle: 0.069570, identity: 0.062966] ETA: 0:00:00.322052"
          ]
        }
      ],
      "source": [
        "# 모델 학습 파이프라인\n",
        "prev_time = time.time()\n",
        "for epoch in range(epoch_start, n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        # (1) Set model input\n",
        "        # dataloader에서 실제 사진 이미지 A와 애니메이션 이미지 B를 배치사이즈만큼 불러옵니다\n",
        "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
        "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
        "\n",
        "        # (2) Adversarial ground truths\n",
        "        # Discriminator의 레이블 값을 만듭니다. valid와 fake변수를 만듭니다. PatchGAN에 따라 valid는 Discriminator의 출력 크기만큼 전부 1로 채워지고,\n",
        "        # 변수 fake는 Discriminator의 출력 크기만큼 전부 0으로 채워집니다.\n",
        "        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n",
        "\n",
        "        # (3) Train Generators\n",
        "        # Generator를 학습합니다. Generator G_AB와 G_BA를 학습 모드로 전환하고, optimizer_G를 zero_grad()합니다.\n",
        "\n",
        "        G_AB.train()\n",
        "        G_BA.train()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # (4) Identity loss\n",
        "        # 색감, 형태 등을 유지하기 위한 Identity loss를 계산합니다. G_BA에 실제 이미지 A를 입력한 후 이를 real_A와 비교하여 L1Loss를 계산합니다.\n",
        "        # 마찬가지로 G_AB에 실제 이미지 B를 입력한 후 이를 다시  real_B와 비교하여 L1Loss를 계산합니다.\n",
        "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "\n",
        "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "\n",
        "        # (5) GAN loss\n",
        "        # GAN Loss를 계산합니다. 이미지 real_A로부터 스타일이 변환된 가짜 이미지 fake_B를 생성합니다. Discriminator D_B는 생성된 fake_B가 진짜인지 가짜인지 분류합니다. 그리고 이 Discriminator D_B를 속이도록 Generator G_AB의 GAN Loss를 계산합니다.\n",
        "        # 이미지 real_B도 마찬가지의 과정을 거쳐서 BA의 GAN Loss를 계산합니다.\n",
        "        fake_B = G_AB(real_A)\n",
        "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
        "        fake_A = G_BA(real_B)\n",
        "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
        "\n",
        "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "        # (6) Cycle loss\n",
        "        # Cyle Loss를 계산합니다. Generator G_AB로 가짜 이미지 fake_B에서 새로운 가짜 이미지 recov_A를 생성합니다. 그리고 이를 다시 원래 실제 이미지 real_A와 비교하여 L1 Loss를 계산합니다.\n",
        "        recov_A = G_BA(fake_B)\n",
        "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
        "        recov_B = G_AB(fake_A)\n",
        "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "\n",
        "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "\n",
        "        # (7) Total loss\n",
        "        # 앞서 계산한 Loss를 총합하여 Generator의 전체 손실함수를 계산하고 Generator의 가중치를 업데이트 합니다.\n",
        "        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
        "\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # (8) Train Discriminator A\n",
        "        # Dicriminator D_A의 가중치를 업데이트 하는 과정입니다. 실제 이미지 A는 valid로 분류하고, 생성한 fake_A는 fake로 분류합니다. 학습을 위하여 optimizer D_A를 zero grad\n",
        "\n",
        "        optimizer_D_A.zero_grad()\n",
        "\n",
        "        # (9) Real loss\n",
        "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
        "        # (10) Fake loss (on batch of previously generated samples)\n",
        "        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
        "        # (11) Total loss\n",
        "        loss_D_A = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "        # (12) Train Discriminator B\n",
        "\n",
        "        optimizer_D_B.zero_grad()\n",
        "\n",
        "        # (13) Real loss\n",
        "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
        "        # (14) Fake loss (on batch of previously generated samples)\n",
        "        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
        "        # (15) Total loss\n",
        "        loss_D_B = (loss_real + loss_fake) / 2\n",
        "\n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "\n",
        "        loss_D = (loss_D_A + loss_D_B) / 2\n",
        "\n",
        "        # (16) Determine approximate time left\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        batches_left = n_epochs * len(dataloader) - batches_done\n",
        "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # (17) Print log\n",
        "        sys.stdout.write(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
        "            % (\n",
        "                epoch,\n",
        "                n_epochs,\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D.item(),\n",
        "                loss_G.item(),\n",
        "                loss_GAN.item(),\n",
        "                loss_cycle.item(),\n",
        "                loss_identity.item(),\n",
        "                time_left,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # (18) If at sample interval save image\n",
        "        if batches_done % sample_interval == 0:\n",
        "            sample_images(batches_done)\n",
        "\n",
        "    # (19) Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_A.step()\n",
        "    lr_scheduler_D_B.step()\n",
        "    # (20) Save model checkpoints\n",
        "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
        "        torch.save(G_AB.state_dict(), \"saved_models/%s/G_AB_%d.pth\" % (dataset_name, epoch))\n",
        "        torch.save(G_BA.state_dict(), \"saved_models/%s/G_BA_%d.pth\" % (dataset_name, epoch))\n",
        "        torch.save(D_A.state_dict(), \"saved_models/%s/D_A_%d.pth\" % (dataset_name, epoch))\n",
        "        torch.save(D_B.state_dict(), \"saved_models/%s/D_B_%d.pth\" % (dataset_name, epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3lD8mvcCQZI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "도성진_Won2Man.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
